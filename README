README 

The data used in this assignment is points.dat. The objective of the assignment was to implement the Expectation-Maximization Algorithm for a Hidden Markov Model. The points.dat  file consists of continuous variables, and thus, probabilities were estimated using a Gaussian distribution. The dataset consists of 2 dimensional data points. 

The Expectation-Maximization Algorithm was implemented because the class-labels are unknown in our training model. As a result, unsupervised learning was implemented through the use of the EM algorithm. The EM algorithm was used to optimize the mus, sigmas, transitions, and initials by running through the forward and backward algorithm in order to calculate the probability of being in a specific hidden state at a time sequence for a given observation. 

REQUIREMENTS:

Please have the numpy and scipy libraries installed and imported to run this program. The use of arrays is necessary for the implementation of the EM algorithm. Iterations of the algorithm require matrix multiplication and array summation. Data for this implementation is stored in /u/cs246/data/em/ 

FUNCTIONS/IMPLEMENTATION:

The parse_data function goes through the points.dat file and extracts the first 90% of the data to the training set, and the other 10% to the validation set. A tuple is returned with the training set and the validation set. 

The init_model initializes parameters mus, sigmas, initial probabilities, and the transition matrix. These parameters will be optimized in the implementation of the expectation-maximization algorithm. As in the previous assignment, this assignment makes use of tied and not-tied cases. If the data is tied, one 2-by-2 sigma matrix will be used for all of the hidden states (clusters). This covariance will be shared between all of the hidden states. The 2-by-2 sigma matrix for the tied case starts as an identity matrix and uses a random value between 2 and 5 which it then assigns to its diagonal values, so that there are no singularities. The initials parameter values are initialized to a fraction of the total hidden states being used. This way the initial probabilities add up to 1. The mus values are initialized by drawing a random value from a Gaussian distribution with an average of 1 and standard deviation of 2. Using a nested loop, each element in the mus matrix will have a random value assigned to it. Lastly, the transitions matrix was initialized by dividing the number 1 for each row element with the amount of hidden states so that they add up to 1. Thus, initially, all the transitions will have the same value, but as EM converges, these values will be optimized. In the case that the model is not tied, each hidden state (cluster) will have its own 2-by-2 sigmas matrix with random values drawn from 1 to 10 and assigned to its diagonals. If the gaussian_hmm_smoketest_clusters.txt file is called, then the mus, sigmas, initials, and transitions will be initialized to those provided in the document. The document assumes 2 hidden states (clusters) for the data. The function then returns the initialized values in a tuple called model. 

The third function, forward, implements the forward algorithm to calculate the alphas. These alphas are the probability of being at a specific hidden state for a given time sequence. The algorithm use the initials and transitions matrix to calculate the alphas. 
At each point, t, and for each hidden state, the emission is calculated and stored in an emission matrix of dimension (length data, clusters). Initially, an alpha matrix of dimension (length of data, clusters) is used to store the updates. The alphas depend on the alpha of the previous time step in the calculation. For each given point, the function normalizes the calculations, so that the values don’t get closer to 0. Thereafter, the log_likelihood is calculated at each point. The function returns the alphas, log_likelihood, and emissions. The emissions will be used in the backward function. 

The fourth function, backward, implements the backward algorithm to calculate betas. A betas matrix is initialized which store the beta calculated at each time-step t. The forward algorithm is called within the betas algorithm, so that it has access to the emissions. The backward algorithm starts from the back and calculates betas by using the previous time step. If the time step is the last observation, then the beta is set to 1, otherwise, it will use the previous time step’s betas for beta calculations at time t. The emission matrix is used in the calculation of betas. The betas are normalized at each step so that they do not go to 0. The algorithm returns the betas. 

The fifth function, train_model unpacks the parameters from the model variable, which holds the mus, sigmas, initials, and transitions. It calls the forward and backward algorithms at each iteration so that the optimized alphas and betas can be used in the calculation of the following parameters: gamma and ksi. To calculate these parameters, the function initializes a gamma matrix with dimensions (length data, clusters) and a ksi matrix with the same dimensions. The gamma_t_k_array is used for the calculations of the mus. By multiplying the observation t by the corresponding gamma value and storing it in the gamma_t_k_array, it is simple to sum over each hidden state (cluster) to use in the calculation of mus. Lastly, a sigmas matrix of dimensions (length data, clusters) is initialized. The sigmas matrix is only used when the sigmas are not tied as it stores the product of the gamma elements by the product of the observation t and mu difference. In the untied case, the sigmas for the kth cluster is calculated by summing over the column of the sigmas_array and dividing by the sum of the column gamma for each k. In the case that the data is tied, sigmas are calculated in a similar way, but by directly adjusting the sigmas value over all summations of observations and clusters. The sigmas is then divided by the length of the data to get the final sigmas. Lastly, to calculate the transitions, the transitions matrix is iterated by both its dimensions for which the ksi matrix is summed over t for each k and j and divided by the sum of gamma over t for each k. The function then returns the model after a given number of iterations. 

The sixth function, average_log_likelihood, calls the forward algorithm to extract the log_likelihood. It divides this value by the total number of data points to get the average log_likelihood. 

The last function extract_parameters, extracts initials, transitions, must, and sigmas from the model and returns them in the order that allows them to be printed correctly in the main function. 

TESTS:

The data includes a validation set and a training set. To run the script without the use of the development test while using the provided cluster file, simply write: ./Yakubov_hmm_gaussian.py —-nodev --iterations ? --clusters_file gaussian_hmm_smoketest_clusters.txt --print_params. Replace the ? question mark with the the iterations requested. To implement the algorithm using the clusters_num and using dev data, simple write:  ./Yakubov_hmm_gaussian.py --iterations ? --cluster_num ?  --print_params, where the question mark should be replaced with the number of iterations and  number of clusters desired, respectively. When running with the tied case, simply write:  ./Yakubov_hmm_gaussian.py --iterations ? --cluster_num ?  --print_params --tied, where the question mark is the number of iterations and number of clusters required, respectively, for the tied case. Please note that the —tied case should not be run using the cluster_file. You will receive an error if doing so. 

Matplotlib was used in the graphic representations. The commented section of the code represents the use of the library for the graphic representations of the average log likelihood vs. training set and dev set. These graphs were analyzed to determine the best number of clusters (hidden states) that should be used in the hidden Markov model. Additionally, overfitting was examined from these graphs. If the code is uncommented, and is run with —nodev suppressed (meaning the —nodev argument is not provided), similar graphs should be observed. Note that some graphs might be different depending on the random initializations of the parameters prior to convergence. 

DISCUSSION 

The Expectation-Maximization Algorithm is implemented in this script using a Hidden Markov Model. The Hidden Markov Model tries to understand the most probable hidden sequences for the observations provided. In our case, the observations are our data at each time sequence, and the hidden states are the clusters. Measuring the average log likelihood determines the average expected counts at each hidden sequence. Thus, as a result, using the Hidden Markov Model provides a strong model in our case, than just a regular expectation maximization implementation for the mixture of gaussians. More is discussed below as to why the Hidden Markov Model is the better model. 

The first graph plotted was using the clusters file which initialized the hidden states (cluster numbers). As observed from the graph, the training data is overfitting. At about 12 iterations, the dev data begins to level off, while the training data’s average log likelihood is increasing. The average log likelihood at about 12 iterations is -4.57 for the dev data, while it is about -4.45 for the training data. As a result, the model can be run for about 10-12 iterations, before the overfitting becomes an issue. The second graph demonstrates this further by using 50 iterations. At about 12 iterations, the dev data average log likelihood is -4.55, while the training data is about -4.45. The training can be cutoff at about 12 iterations. 


The third graph is 10 iterations using 2 clusters for a non-tied case. Using 10 iterations is again about enough before the model begins to overfit. The dev data average log-likelihood is about -4.45, while the training data is greater than -4.4 after 10 iterations. 

The fourth graph represents 3 clusters for a non-tied case using 10 iterations. The average log-likelihood for the dev data is about -4.05 after 10 iterations, while the train data is about -4.0. This scenario provides a better average log-likelihood than using 2 clusters, however, we must examine a few more cluster numbers, before determining which one is most optimal. 

The fifth graph represents 4 clusters for a non-tied case using 10 iterations. The average log-likelihood for the dev data after 10 iterations is roughly higher than -3.8. The training data’s average log likelihood is about the same. While there is some overfitting observed, it would be best to run the algorithm for about 10 iterations in order for it to converge. The average log likelihood looks better using 4 clusters for a non-tied case, with respect to 3 clusters. 

The sixth graph represents 5 clusters for a non-tied case using 10 iterations. The average log-likelihood after 10 iterations for the dev data is about -4.05 while the average log likelihood for the training data after 10 iterations is about -4.0. Again, running the algorithm for 10 iterations is best to stop overfitting. 

From these results, it would be best to assume that 3-4 clusters is optimal for this dataset when running the hidden Markov model. The average log-likelihood was about the best using this interval, and overfitting could be controlled for by running the algorithm for 10 iterations. 

The seventh graph represents using 2 clusters for a tied case of 10 iterations. The average log-likelihood after 10 iterations for dev data is -4.45, while the training data is about -4.50. If this was run for more iterations it would seem that the dev data would begin to decrease, while the training data would increase. 

For the eighth graph, 3 clusters in a tied case was used for 10 iterations. The dev data begins to drop after about 3-4 iterations, while the training data continues. This represents overfitting. The average log-likelihood at about 3 iterations for the dev data is about -4.53, while the training data is relatively the same for this number of iterations. This is a good cutoff against overfitting. 

For the ninth graph, 4 clusters is implemented using a tied case in 10 iterations. About 6 iterations is a good cutoff against overfitting. The average log likelihood at this iteration for the dev data is about -4.42, while the training data has an average log likelihood of about -4.40. 

Lastly, the tenth graph shows average log-likelihood for 5 clusters at 10 iterations in a tied case. The average log-likelihood converges to about -4.1 for both dev data and the training data after 10 iterations. From these results, it would be best to assume that 5 clusters is best for the tied case. The average log-likelihood is best, and it seems that 10 iterations is still best to stop any overfitting. 

In conclusion, the Hidden Markov Model provides a better model than the Mixture of Gaussians in the Expectation-Maximization implementation. In the Mixture of Gaussians EM model, the best results were provided with 3 clusters, where the average log-likelihood was about -4.45 for the non-tied case and -4.67 for the tied case. Using HMM in EM provided better average log-likelihood results, and thus, is a stronger model.The best hidden states were at 3 or 4 for the HMM EM. 





